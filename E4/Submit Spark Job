
# ====================================================================
# INGEST DATA KAFKA
# ====================================================================

spark-submit \
    --class org.apache.hudi.utilities.streamer.HoodieStreamer \
    --packages org.apache.hudi:hudi-spark3.4-bundle_2.12:0.14.0 \
    --properties-file spark-config.properties \
    --master 'local[*]' \
    --executor-memory 1g \
    /Users/soumilshah/IdeaProjects/SparkProject/DeltaStreamer/jar/hudi-utilities-slim-bundle_2.12-0.14.0.jar \
     --table-type MERGE_ON_READ \
    --op UPSERT \
    --source-ordering-field ts \
    --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \
    --target-base-path 'file:///Users/soumilshah/IdeaProjects/SparkProject/DeltaStreamer/hudi/bronze_orders'  \
    --target-table orders \
    --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \
    --disable-compaction \
    --props hudi_tbl.props

========
RUN compaction
=======

spark-submit \
    --class org.apache.hudi.utilities.HoodieCompactor \
    --packages 'org.apache.hudi:hudi-spark3.4-bundle_2.12:0.14.0' \
    --master 'local[*]' \
    --executor-memory 1g \
    /Users/soumilshah/IdeaProjects/SparkProject/DeltaStreamer/jar/hudi-utilities-slim-bundle_2.12-0.14.0.jar \
    --base-path file:///Users/soumilshah/IdeaProjects/SparkProject/DeltaStreamer/hudi/bronze_orders \
    --table-name bronze_orders \
    --mode scheduleAndExecute \
    --strategy org.apache.hudi.table.action.compact.strategy.LogFileSizeBasedCompactionStrategy \
    --hoodie-conf hoodie.compact.inline.max.delta.commits=1 \
    --parallelism 2 \
    --spark-memory 2g

MODE execute | schedule  | scheduleAndExecute
https://github.com/apache/hudi/blob/master/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java


